{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746bb70-eb1f-473e-bc2b-97abcf5067f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from vit_llama_model import SingleImageTransformerCLIP_LLaMA\n",
    "from dataset import BarLinkageDataset \n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "from dataset_generation.curve_plot import get_pca_inclination, rotate_curve\n",
    "import scipy.spatial.distance as sciDist\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354fc90-9448-4531-ad0b-f95058e8fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headless simulator version\n",
    "index = 0 # local server index \n",
    "API_ENDPOINT = f\"http://localhost:4000/simulation\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "speedscale = 1\n",
    "steps = 360\n",
    "minsteps = int(steps*20/360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ae72c-963c-458f-a536-c6737e88084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./weights/transformer_weights_VIT_LLAMA/A100_LLAMA_d256_h8_n6_bs512_lr0.0001_vit_llama.pth\"\n",
    "data_dir = \"/home/anurizada/Documents/processed_dataset_17\"\n",
    "batch_size = 1\n",
    "\n",
    "dataset = BarLinkageDataset(data_dir=data_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4913484-58cd-4236-bc77-e3cf855f884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model_config = checkpoint['model_config']\n",
    "\n",
    "# Initialize model\n",
    "model = SingleImageTransformerCLIP_LLaMA(\n",
    "    tgt_seq_len=model_config['tgt_seq_len'],\n",
    "    d_model=model_config['d_model'],\n",
    "    h=model_config['h'],\n",
    "    N=model_config['N'],\n",
    "    num_labels=model_config['num_labels'],\n",
    "    vocab_size=model_config['vocab_size']).to(device)\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# ---------------------------\n",
    "# Count parameters\n",
    "# ---------------------------\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "\n",
    "    print(\"\\nðŸ§® Model Parameter Summary\")\n",
    "    print(f\"Total parameters:     {total_params:,}  ({total_params/1e6:.2f} M)\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}  ({trainable_params/1e6:.2f} M)\")\n",
    "    print(f\"Frozen parameters:    {non_trainable_params:,}  ({non_trainable_params/1e6:.2f} M)\")\n",
    "    return total_params, trainable_params, non_trainable_params\n",
    "\n",
    "# Run the counter\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ad52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "# ===============================\n",
    "# CONFIG\n",
    "# ===============================\n",
    "MAX_SAMPLES = 100\n",
    "SOS_TOKEN, EOS_TOKEN, PAD_TOKEN = 0, 1, 2\n",
    "NUM_SPECIAL_TOKENS = 3\n",
    "NUM_MECH_TYPES = 17\n",
    "BIN_OFFSET = NUM_SPECIAL_TOKENS\n",
    "\n",
    "# model, model_config, dataloader, device must be defined externally\n",
    "# API_ENDPOINT, HEADERS, speedscale, steps, minsteps also defined externally\n",
    "\n",
    "label_mapping_path = \"/home/anurizada/Documents/processed_dataset_17/label_mapping.json\"\n",
    "coupler_mapping_path = \"/home/anurizada/Documents/transformer/BSIdict.json\"\n",
    "\n",
    "with open(label_mapping_path, \"r\") as f:\n",
    "    label_mapping = json.load(f)\n",
    "index_to_label = label_mapping[\"index_to_label\"]\n",
    "\n",
    "with open(coupler_mapping_path, \"r\") as f:\n",
    "    coupler_mapping = json.load(f)\n",
    "\n",
    "\n",
    "def coupler_index_for(mech_type: str) -> int:\n",
    "    \"\"\"Return coupler curve index from BSIdict.json.\"\"\"\n",
    "    if mech_type in coupler_mapping and \"c\" in coupler_mapping[mech_type]:\n",
    "        cvec = coupler_mapping[mech_type][\"c\"]\n",
    "        if isinstance(cvec, list) and 1 in cvec:\n",
    "            return cvec.index(1)\n",
    "    return -1\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# BINNER\n",
    "# ===============================\n",
    "class CoordinateBinner:\n",
    "    def __init__(self, kappa=1.0, num_bins=200):\n",
    "        self.kappa = kappa\n",
    "        self.num_bins = num_bins\n",
    "        self.bin_edges = np.linspace(-kappa, kappa, num_bins + 1)\n",
    "        self.bin_centers = (self.bin_edges[:-1] + self.bin_edges[1:]) / 2\n",
    "\n",
    "    def bin_to_value_torch(self, bin_index_tensor):\n",
    "        bin_index_tensor = torch.clamp(bin_index_tensor, 0, self.num_bins - 1)\n",
    "        device = bin_index_tensor.device\n",
    "        bin_centers_tensor = torch.tensor(self.bin_centers, device=device, dtype=torch.float32)\n",
    "        return bin_centers_tensor[bin_index_tensor]\n",
    "\n",
    "\n",
    "binner = CoordinateBinner(kappa=1.0, num_bins=201)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# AUTOREGRESSIVE PREDICTION (LLaMA)\n",
    "# ===============================\n",
    "def predict_autoregressive_llama(\n",
    "    model,\n",
    "    image_data,          # (1, C, H, W)\n",
    "    mech_idx,            # int\n",
    "    max_seq_len,         # target sequence length\n",
    "    device,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    eos_token=EOS_TOKEN,\n",
    "    sos_token=SOS_TOKEN,\n",
    "):\n",
    "    \"\"\"\n",
    "    Autoregressive decoding for LLaMA-style transformer using image input.\n",
    "    Uses the modelâ€™s internal causal mask.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        decoder_input = torch.tensor([[sos_token]], device=device, dtype=torch.long)\n",
    "        mech_labels = torch.tensor([mech_idx], device=device, dtype=torch.long)\n",
    "\n",
    "        for step in range(max_seq_len):\n",
    "            # Model handles causal masking internally\n",
    "            logits = model(decoder_input, None, image_data, mech_labels)\n",
    "            next_logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "\n",
    "            # Sampling strategy\n",
    "            if top_k is not None and top_k > 0:\n",
    "                k = min(int(top_k), probs.size(-1))\n",
    "                topk_probs, topk_idx = torch.topk(probs, k=k, dim=-1)\n",
    "                next_token = topk_idx.gather(-1, torch.multinomial(topk_probs, 1))\n",
    "            elif temperature == 0:\n",
    "                next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            token = int(next_token.item())\n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "\n",
    "            if token == eos_token:\n",
    "                break\n",
    "\n",
    "    return decoder_input.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# MAIN LOOP\n",
    "# ===============================\n",
    "print(\"Starting conditional coupler curve generation (image-based)...\")\n",
    "os.makedirs(\"results_coupler\", exist_ok=True)\n",
    "\n",
    "for i, batch in enumerate(tqdm(dataloader, total=MAX_SAMPLES, desc=\"Simulating\")):\n",
    "    if i >= MAX_SAMPLES:\n",
    "        break\n",
    "\n",
    "    # --- Image input ---\n",
    "    image_data = batch[\"images\"].to(device)  # (B, C, H, W)\n",
    "    gt_tokens = batch[\"labels_discrete\"][0].numpy()\n",
    "\n",
    "    gt_mech_idx = int(batch[\"encoded_labels\"][0].item())\n",
    "    gt_mech_name = index_to_label.get(str(gt_mech_idx), \"UNKNOWN\")\n",
    "\n",
    "    # --- Save input image ---\n",
    "    sample_dir = f\"results_coupler/sample_{i:03d}_{gt_mech_name}\"\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "    img_np = image_data[0].detach().cpu().squeeze().numpy()\n",
    "    plt.imsave(os.path.join(sample_dir, \"input_image.png\"), img_np, cmap=\"gray\")\n",
    "    print(f\"ðŸ–¼ï¸ Saved input image: {sample_dir}/input_image.png\")\n",
    "\n",
    "    # --- Ground Truth coordinates ---\n",
    "    gt_coord_tokens = [t for t in gt_tokens if t >= BIN_OFFSET]\n",
    "    if len(gt_coord_tokens) < 4:\n",
    "        continue\n",
    "    gt_coords_tensor = torch.tensor(gt_coord_tokens) - BIN_OFFSET\n",
    "    gt_coords_float = binner.bin_to_value_torch(gt_coords_tensor).cpu().numpy()\n",
    "    if gt_coords_float.size % 2 == 1:\n",
    "        gt_coords_float = gt_coords_float[:-1]\n",
    "    gt_points = gt_coords_float.reshape(-1, 2)\n",
    "\n",
    "    # --- Simulate GT coupler ---\n",
    "    ex_gt = {\n",
    "        \"params\": gt_points.tolist(),\n",
    "        \"type\": gt_mech_name,\n",
    "        \"speedScale\": speedscale,\n",
    "        \"steps\": steps,\n",
    "        \"relativeTolerance\": 0.1,\n",
    "    }\n",
    "    try:\n",
    "        temp = requests.post(url=API_ENDPOINT, headers=HEADERS, data=json.dumps([ex_gt])).json()\n",
    "        P = np.array(temp[0][\"poses\"]) if isinstance(temp, list) and temp and \"poses\" in temp[0] else None\n",
    "    except Exception as e:\n",
    "        print(f\"GT sim failed: {e}\")\n",
    "        continue\n",
    "\n",
    "    if P is None or P.shape[0] < minsteps:\n",
    "        continue\n",
    "\n",
    "    coup_idx_gt = coupler_index_for(gt_mech_name)\n",
    "    if coup_idx_gt < 0:\n",
    "        continue\n",
    "\n",
    "    original_x, original_y = P[:, coup_idx_gt, 0], P[:, coup_idx_gt, 1]\n",
    "    orig_phi = -get_pca_inclination(original_x, original_y)\n",
    "    orig_denom = np.sqrt(np.var(original_x) + np.var(original_y)) + 1e-8\n",
    "    ox_mean, oy_mean = np.mean(original_x), np.mean(original_y)\n",
    "\n",
    "    print(f\"\\n=== Sample {i} | GT={gt_mech_name} ===\")\n",
    "\n",
    "    all_predicted_points = {}\n",
    "\n",
    "    # --- Predict for all mechanism types ---\n",
    "    for mech_idx in range(NUM_MECH_TYPES):\n",
    "        mech_name = index_to_label.get(str(mech_idx), f\"mech_{mech_idx}\")\n",
    "        single_image = image_data[0].unsqueeze(0)  # (1, C, H, W)\n",
    "\n",
    "        pred_tokens = predict_autoregressive_llama(\n",
    "            model=model,\n",
    "            image_data=single_image,\n",
    "            mech_idx=mech_idx,\n",
    "            max_seq_len=model_config[\"tgt_seq_len\"],\n",
    "            device=device,\n",
    "            temperature=0.0,\n",
    "            top_k=None,\n",
    "        )\n",
    "\n",
    "        coord_tokens = [t for t in pred_tokens if t >= BIN_OFFSET]\n",
    "        if len(coord_tokens) < 4:\n",
    "            continue\n",
    "        coords_tensor = torch.tensor(coord_tokens) - BIN_OFFSET\n",
    "        coords_float = binner.bin_to_value_torch(coords_tensor).cpu().numpy()\n",
    "        if coords_float.size % 2 == 1:\n",
    "            coords_float = coords_float[:-1]\n",
    "        pred_points = coords_float.reshape(-1, 2)\n",
    "        all_predicted_points[mech_name] = pred_points\n",
    "\n",
    "        # --- Simulate predicted coupler ---\n",
    "        ex_pred = {\n",
    "            \"params\": pred_points.tolist(),\n",
    "            \"type\": mech_name,\n",
    "            \"speedScale\": speedscale,\n",
    "            \"steps\": steps,\n",
    "            \"relativeTolerance\": 0.1,\n",
    "        }\n",
    "        try:\n",
    "            temp = requests.post(url=API_ENDPOINT, headers=HEADERS, data=json.dumps([ex_pred])).json()\n",
    "            Pp = np.array(temp[0][\"poses\"]) if isinstance(temp, list) and temp and \"poses\" in temp[0] else None\n",
    "        except Exception as e:\n",
    "            print(f\"Pred sim failed for {mech_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if Pp is None or P.shape[0] < minsteps:\n",
    "            continue\n",
    "\n",
    "        coup_idx_pred = coupler_index_for(mech_name)\n",
    "        if coup_idx_pred < 0:\n",
    "            continue\n",
    "        generated_x, generated_y = Pp[:, coup_idx_pred, 0], Pp[:, coup_idx_pred, 1]\n",
    "        if np.isnan(generated_x).any() or np.isinf(generated_x).any():\n",
    "            continue\n",
    "\n",
    "        # --- Align predicted coupler to GT ---\n",
    "        gen_phi = -get_pca_inclination(generated_x, generated_y)\n",
    "        rotation = gen_phi - orig_phi\n",
    "        generated_x, generated_y = rotate_curve(generated_x, generated_y, rotation)\n",
    "        gen_denom = np.sqrt(np.var(generated_x) + np.var(generated_y)) + 1e-8\n",
    "        scale = orig_denom / gen_denom\n",
    "        generated_x *= scale\n",
    "        generated_y *= scale\n",
    "        gx_mean, gy_mean = np.mean(generated_x), np.mean(generated_y)\n",
    "        generated_x -= (gx_mean - ox_mean)\n",
    "        generated_y -= (gy_mean - oy_mean)\n",
    "\n",
    "        # --- Plot GT vs predicted coupler ---\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.plot(original_x, original_y, \"r-\", label=f\"GT Coupler ({gt_mech_name})\")\n",
    "        plt.plot(generated_x, generated_y, \"g--\", label=f\"Pred Coupler ({mech_name})\")\n",
    "        plt.scatter(gt_points[:, 0], gt_points[:, 1], color=\"red\", s=40, zorder=5)\n",
    "        plt.scatter(pred_points[:, 0], pred_points[:, 1], color=\"green\", s=40, zorder=5)\n",
    "        plt.axis(\"equal\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Sample {i} | GT={gt_mech_name} | Pred={mech_name}\")\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(sample_dir, f\"mech_{mech_name}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "    # ============================================================\n",
    "    # Combined joint prediction scatter plot (color-coded by joint index)\n",
    "    # ============================================================\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(gt_points[:, 0], gt_points[:, 1], c=\"red\", s=80, edgecolor=\"black\",\n",
    "                label=f\"GT ({gt_mech_name})\", zorder=6)\n",
    "    for j, (x, y) in enumerate(gt_points):\n",
    "        plt.text(x + 0.005, y + 0.005, f\"{j}\", color=\"red\", fontsize=9, weight=\"bold\")\n",
    "\n",
    "    max_joints = max(pts.shape[0] for pts in all_predicted_points.values())\n",
    "    cmap = plt.cm.get_cmap(\"tab10\", max_joints)\n",
    "\n",
    "    for mech_name, pts in all_predicted_points.items():\n",
    "        num_joints = pts.shape[0]\n",
    "        for j in range(num_joints):\n",
    "            color = cmap(j % max_joints)\n",
    "            plt.scatter(pts[j, 0], pts[j, 1], color=color, s=30, alpha=0.8)\n",
    "            plt.text(pts[j, 0] + 0.002, pts[j, 1] + 0.002, str(j), fontsize=7, color=color, alpha=0.9)\n",
    "\n",
    "    plt.title(f\"Predicted Joint Positions â€” Sample {i} (GT={gt_mech_name})\")\n",
    "    plt.xlabel(\"X coordinate\")\n",
    "    plt.ylabel(\"Y coordinate\")\n",
    "    plt.axis(\"equal\")\n",
    "    handles = [\n",
    "        plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=cmap(j), label=f\"Joint {j}\")\n",
    "        for j in range(max_joints)\n",
    "    ]\n",
    "    plt.legend(handles=handles, fontsize=7, loc=\"upper right\", ncol=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    joint_scatter_path = os.path.join(sample_dir, \"all_predicted_joints_colored.png\")\n",
    "    plt.savefig(joint_scatter_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"âœ… Saved: {joint_scatter_path}\")\n",
    "\n",
    "print(\"âœ… Finished all mechanism variations.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
